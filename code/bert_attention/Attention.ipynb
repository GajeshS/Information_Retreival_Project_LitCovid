{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingAttention(nn.Module):\n",
    "    def __init__(self, num_input_features, num_hidden_features):\n",
    "        super(EmbeddingAttention,self).__init__()\n",
    "        self.l1 = nn.Linear(num_input_features,num_hidden_features)\n",
    "        self.act_1 = nn.LeakyReLU()\n",
    "        self.l2 = nn.Linear(num_hidden_features, 1) # the final attention weight for the input\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.attention_weights = torch.zeros((1,1))\n",
    "    \n",
    "    def getAttentionWeights(self):\n",
    "        return self.attention_weights\n",
    "\n",
    "    def forward(self,x): # input format ==> (m,num_input_features)\n",
    "        l1_out = self.l1(x)\n",
    "        act1_out = self.act_1(l1_out)\n",
    "        l2_out = self.l2(act1_out)\n",
    "        self.attention_weights = self.sigmoid(l2_out) # this would be (m,1) dimensional\n",
    "        return torch.mul(self.attention_weights,x) # broadcasting will happen so final result is elementwide multiplication of (m,1) and (m,num_features) == (m,num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ea = EmbeddingAttention(5,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5139],\n",
       "        [0.5141]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ea.getAttentionWeights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5139, 1.0279, 1.5418, 2.0558, 2.5697],\n",
       "        [0.5141, 2.5705, 1.5423, 2.0564, 2.5705]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ea.forward(torch.Tensor([[1,2,3,4,5],[1,5,3,4,5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4075595ffcf76b6a2ce9721aa897b8612cee89dfb18a3ef9e70c4d2bb1cb64b3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
