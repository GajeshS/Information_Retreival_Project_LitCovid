 0%
48/23960 [01:35<12:44:11, 1.92s/it]
Epoch 40 of 40: 100%	40/40 [9:31:24<00:00, 850.80s/it]
Epochs 0/40. Running Loss: 0.2270: 100%		2995/2995 [14:24<00:00, 3.46it/s]
Epochs 1/40. Running Loss: 0.1139: 100%		2995/2995 [14:25<00:00, 3.48it/s]

/opt/conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:922: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  model.parameters(), args.max_grad_norm

Epochs 2/40. Running Loss: 0.1097: 100%		2995/2995 [14:30<00:00, 3.46it/s]
Epochs 3/40. Running Loss: 0.0787: 100%		2995/2995 [14:24<00:00, 3.47it/s]
Epochs 4/40. Running Loss: 0.1881: 100%		2995/2995 [14:28<00:00, 3.47it/s]
Epochs 5/40. Running Loss: 0.1181: 100%		2995/2995 [14:24<00:00, 3.48it/s]
Epochs 6/40. Running Loss: 0.0443: 100%		2995/2995 [14:25<00:00, 3.49it/s]
Epochs 7/40. Running Loss: 0.1739: 100%		2995/2995 [14:21<00:00, 3.48it/s]
Epochs 8/40. Running Loss: 0.0331: 100%		2995/2995 [14:22<00:00, 3.46it/s]
Epochs 9/40. Running Loss: 0.0109: 100%		2995/2995 [14:19<00:00, 3.46it/s]
Epochs 10/40. Running Loss: 0.0334: 100%	2995/2995 [14:22<00:00, 3.49it/s]
Epochs 11/40. Running Loss: 0.0007: 100%	2995/2995 [14:18<00:00, 3.51it/s]
Epochs 12/40. Running Loss: 0.0104: 100%	2995/2995 [14:19<00:00, 3.48it/s]
Epochs 13/40. Running Loss: 0.0005: 100%	2995/2995 [14:16<00:00, 3.51it/s]
Epochs 14/40. Running Loss: 0.0016: 100%	2995/2995 [14:17<00:00, 3.50it/s]
Epochs 15/40. Running Loss: 0.0061: 100%	2995/2995 [14:14<00:00, 3.49it/s]
Epochs 16/40. Running Loss: 0.0011: 100%	2995/2995 [14:16<00:00, 3.52it/s]
Epochs 17/40. Running Loss: 0.0002: 100%	2995/2995 [14:13<00:00, 3.53it/s]
Epochs 18/40. Running Loss: 0.0007: 100%	2995/2995 [14:14<00:00, 3.52it/s]
Epochs 19/40. Running Loss: 0.0001: 100%	2995/2995 [14:11<00:00, 3.52it/s]
Epochs 20/40. Running Loss: 0.0001: 100%	2995/2995 [14:13<00:00, 3.54it/s]
Epochs 21/40. Running Loss: 0.0002: 100%	2995/2995 [14:10<00:00, 3.54it/s]
Epochs 22/40. Running Loss: 0.0003: 100%	2995/2995 [14:12<00:00, 3.53it/s]
Epochs 23/40. Running Loss: 0.0079: 100%	2995/2995 [14:09<00:00, 3.52it/s]
Epochs 24/40. Running Loss: 0.0001: 100%	2995/2995 [14:11<00:00, 3.53it/s]
Epochs 25/40. Running Loss: 0.0001: 100%	2995/2995 [14:09<00:00, 3.53it/s]
Epochs 26/40. Running Loss: 0.0002: 100%	2995/2995 [14:10<00:00, 3.52it/s]
Epochs 27/40. Running Loss: 0.0001: 100%	2995/2995 [14:09<00:00, 3.53it/s]
Epochs 28/40. Running Loss: 0.0001: 100%	2995/2995 [14:11<00:00, 3.52it/s]
Epochs 29/40. Running Loss: 0.0000: 100%	2995/2995 [14:08<00:00, 3.54it/s]
Epochs 30/40. Running Loss: 0.0000: 100%	2995/2995 [14:10<00:00, 3.54it/s]
Epochs 31/40. Running Loss: 0.0001: 100%	2995/2995 [14:08<00:00, 3.55it/s]
Epochs 32/40. Running Loss: 0.0000: 100%	2995/2995 [14:10<00:00, 3.53it/s]
Epochs 33/40. Running Loss: 0.0001: 100%	2995/2995 [14:08<00:00, 3.54it/s]
Epochs 34/40. Running Loss: 0.0000: 100%	2995/2995 [14:09<00:00, 3.57it/s]
Epochs 35/40. Running Loss: 0.0000: 100%	2995/2995 [14:06<00:00, 3.56it/s]
Epochs 36/40. Running Loss: 0.0001: 100%	2995/2995 [14:08<00:00, 3.54it/s]
Epochs 37/40. Running Loss: 0.0000: 100%	2995/2995 [14:06<00:00, 3.54it/s]
Epochs 38/40. Running Loss: 0.0001: 100%	2995/2995 [14:08<00:00, 3.54it/s]
Epochs 39/40. Running Loss: 0.0000: 100%	2995/2995 [14:10<00:00, 3.49it/s]

(119800, 0.033227868420262666)